{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 09:50:38.924872 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0702 09:50:38.958184 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0702 09:50:38.984549 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 09:51:45.362443 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0702 09:51:45.363057 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0702 09:51:45.517411 140666085185344 deprecation_wrapper.py:119] From /home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neural/.local/share/virtualenvs/reinforcement-V9xGC4i1/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 1.934s, episode steps: 79, steps per second: 41, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.427437, mean_absolute_error: 0.495165, mean_q: 0.053262\n",
      "  113/5000: episode: 2, duration: 0.560s, episode steps: 34, steps per second: 61, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.353533, mean_absolute_error: 0.446728, mean_q: 0.190409\n",
      "  163/5000: episode: 3, duration: 0.835s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.313008, mean_absolute_error: 0.462915, mean_q: 0.319764\n",
      "  197/5000: episode: 4, duration: 0.567s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.278897, mean_absolute_error: 0.501364, mean_q: 0.467233\n",
      "  261/5000: episode: 5, duration: 1.067s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.394, 0.861], loss: 0.228939, mean_absolute_error: 0.557247, mean_q: 0.681407\n",
      "  295/5000: episode: 6, duration: 0.568s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.181826, mean_absolute_error: 0.640173, mean_q: 0.936033\n",
      "  327/5000: episode: 7, duration: 0.529s, episode steps: 32, steps per second: 61, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.350, 0.979], loss: 0.143071, mean_absolute_error: 0.703051, mean_q: 1.138208\n",
      "  354/5000: episode: 8, duration: 0.447s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.078 [-0.399, 0.823], loss: 0.126537, mean_absolute_error: 0.789643, mean_q: 1.343260\n",
      "  374/5000: episode: 9, duration: 0.334s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.542, 0.929], loss: 0.103115, mean_absolute_error: 0.845831, mean_q: 1.523963\n",
      "  392/5000: episode: 10, duration: 0.296s, episode steps: 18, steps per second: 61, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.415, 0.857], loss: 0.100253, mean_absolute_error: 0.911809, mean_q: 1.675348\n",
      "  413/5000: episode: 11, duration: 0.360s, episode steps: 21, steps per second: 58, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.083 [-0.438, 1.059], loss: 0.085987, mean_absolute_error: 0.974646, mean_q: 1.833758\n",
      "  434/5000: episode: 12, duration: 0.342s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.369, 1.041], loss: 0.086042, mean_absolute_error: 1.051250, mean_q: 2.008224\n",
      "  457/5000: episode: 13, duration: 0.379s, episode steps: 23, steps per second: 61, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.083 [-0.543, 0.988], loss: 0.084736, mean_absolute_error: 1.158095, mean_q: 2.209956\n",
      "  475/5000: episode: 14, duration: 0.303s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.088 [-0.583, 1.207], loss: 0.105387, mean_absolute_error: 1.239268, mean_q: 2.382119\n",
      "  489/5000: episode: 15, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.219], loss: 0.095651, mean_absolute_error: 1.295969, mean_q: 2.531597\n",
      "  501/5000: episode: 16, duration: 0.194s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.779, 1.336], loss: 0.114062, mean_absolute_error: 1.377266, mean_q: 2.662908\n",
      "  517/5000: episode: 17, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.776, 1.451], loss: 0.155916, mean_absolute_error: 1.462337, mean_q: 2.782054\n",
      "  534/5000: episode: 18, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.090 [-0.804, 1.479], loss: 0.143008, mean_absolute_error: 1.480306, mean_q: 2.896568\n",
      "  547/5000: episode: 19, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.121 [-0.766, 1.414], loss: 0.113505, mean_absolute_error: 1.551631, mean_q: 3.063325\n",
      "  559/5000: episode: 20, duration: 0.194s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.980, 1.698], loss: 0.238622, mean_absolute_error: 1.648163, mean_q: 3.184683\n",
      "  569/5000: episode: 21, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.146 [-0.771, 1.498], loss: 0.211660, mean_absolute_error: 1.704541, mean_q: 3.276513\n",
      "  580/5000: episode: 22, duration: 0.172s, episode steps: 11, steps per second: 64, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-0.998, 1.756], loss: 0.207120, mean_absolute_error: 1.749732, mean_q: 3.382689\n",
      "  589/5000: episode: 23, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-1.141, 1.974], loss: 0.180281, mean_absolute_error: 1.767713, mean_q: 3.501749\n",
      "  598/5000: episode: 24, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.357, 2.221], loss: 0.314883, mean_absolute_error: 1.874652, mean_q: 3.603350\n",
      "  608/5000: episode: 25, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.969, 3.033], loss: 0.236939, mean_absolute_error: 1.889815, mean_q: 3.695432\n",
      "  618/5000: episode: 26, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.554, 2.619], loss: 0.314025, mean_absolute_error: 1.961148, mean_q: 3.800681\n",
      "  628/5000: episode: 27, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 0.273481, mean_absolute_error: 1.980341, mean_q: 3.858484\n",
      "  637/5000: episode: 28, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.810, 2.873], loss: 0.369859, mean_absolute_error: 2.049859, mean_q: 3.971043\n",
      "  647/5000: episode: 29, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.166 [-1.521, 2.537], loss: 0.401320, mean_absolute_error: 2.096054, mean_q: 4.066297\n",
      "  657/5000: episode: 30, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.524, 2.564], loss: 0.298923, mean_absolute_error: 2.157754, mean_q: 4.169152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  670/5000: episode: 31, duration: 0.221s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.106 [-1.597, 2.480], loss: 0.354126, mean_absolute_error: 2.191872, mean_q: 4.297133\n",
      "  679/5000: episode: 32, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.611, 2.534], loss: 0.647125, mean_absolute_error: 2.366986, mean_q: 4.470751\n",
      "  688/5000: episode: 33, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.541, 2.425], loss: 0.379106, mean_absolute_error: 2.276259, mean_q: 4.408703\n",
      "  697/5000: episode: 34, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.712, 2.836], loss: 0.462833, mean_absolute_error: 2.343694, mean_q: 4.533526\n",
      "  706/5000: episode: 35, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.397, 2.317], loss: 0.688176, mean_absolute_error: 2.446572, mean_q: 4.620378\n",
      "  716/5000: episode: 36, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.119 [-1.588, 2.485], loss: 0.526586, mean_absolute_error: 2.465027, mean_q: 4.619959\n",
      "  728/5000: episode: 37, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.118 [-1.549, 2.480], loss: 0.551631, mean_absolute_error: 2.474317, mean_q: 4.735565\n",
      "  737/5000: episode: 38, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.354, 2.301], loss: 0.659399, mean_absolute_error: 2.566337, mean_q: 4.885706\n",
      "  747/5000: episode: 39, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.568, 2.471], loss: 0.458941, mean_absolute_error: 2.576874, mean_q: 4.921107\n",
      "  758/5000: episode: 40, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.106 [-1.387, 2.281], loss: 0.781411, mean_absolute_error: 2.653726, mean_q: 5.051390\n",
      "  766/5000: episode: 41, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.163 [-1.368, 2.236], loss: 0.546606, mean_absolute_error: 2.655085, mean_q: 5.063749\n",
      "  775/5000: episode: 42, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.124 [-1.420, 2.234], loss: 0.365582, mean_absolute_error: 2.640267, mean_q: 5.155621\n",
      "  783/5000: episode: 43, duration: 0.126s, episode steps: 8, steps per second: 64, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.168 [-1.333, 2.247], loss: 0.398491, mean_absolute_error: 2.667996, mean_q: 5.284882\n",
      "  793/5000: episode: 44, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.526, 2.492], loss: 0.844763, mean_absolute_error: 2.819793, mean_q: 5.405150\n",
      "  802/5000: episode: 45, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.162 [-1.525, 2.498], loss: 0.812927, mean_absolute_error: 2.843360, mean_q: 5.394068\n",
      "  811/5000: episode: 46, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.176 [-1.338, 2.324], loss: 0.662218, mean_absolute_error: 2.812282, mean_q: 5.367758\n",
      "  821/5000: episode: 47, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.141 [-1.139, 1.954], loss: 0.894286, mean_absolute_error: 2.920840, mean_q: 5.387050\n",
      "  831/5000: episode: 48, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.212, 1.985], loss: 0.591435, mean_absolute_error: 2.924805, mean_q: 5.479439\n",
      "  841/5000: episode: 49, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.186, 1.988], loss: 0.717375, mean_absolute_error: 2.951165, mean_q: 5.673359\n",
      "  852/5000: episode: 50, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.092 [-1.192, 1.728], loss: 0.751639, mean_absolute_error: 3.001171, mean_q: 5.695033\n",
      "  863/5000: episode: 51, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-0.951, 1.695], loss: 0.703030, mean_absolute_error: 3.034712, mean_q: 5.727269\n",
      "  875/5000: episode: 52, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-1.204, 1.748], loss: 0.885228, mean_absolute_error: 3.142031, mean_q: 5.798095\n",
      "  889/5000: episode: 53, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-1.024, 1.643], loss: 0.836860, mean_absolute_error: 3.126250, mean_q: 5.869308\n",
      "  900/5000: episode: 54, duration: 0.176s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.379, 2.278], loss: 0.588612, mean_absolute_error: 3.101565, mean_q: 5.987619\n",
      "  909/5000: episode: 55, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-1.349, 2.132], loss: 0.769809, mean_absolute_error: 3.201741, mean_q: 6.070466\n",
      "  921/5000: episode: 56, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.090 [-1.011, 1.621], loss: 0.733550, mean_absolute_error: 3.231324, mean_q: 6.171017\n",
      "  931/5000: episode: 57, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.117 [-0.996, 1.639], loss: 0.851486, mean_absolute_error: 3.294043, mean_q: 6.200183\n",
      "  943/5000: episode: 58, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.106 [-0.951, 1.571], loss: 0.728004, mean_absolute_error: 3.273872, mean_q: 6.248835\n",
      "  953/5000: episode: 59, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-0.830, 1.512], loss: 0.687182, mean_absolute_error: 3.332867, mean_q: 6.333597\n",
      "  966/5000: episode: 60, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.832, 1.415], loss: 1.029224, mean_absolute_error: 3.424844, mean_q: 6.354682\n",
      "  978/5000: episode: 61, duration: 0.195s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-0.993, 1.573], loss: 0.840628, mean_absolute_error: 3.427370, mean_q: 6.434915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  990/5000: episode: 62, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-0.978, 1.649], loss: 1.056050, mean_absolute_error: 3.501848, mean_q: 6.507382\n",
      " 1004/5000: episode: 63, duration: 0.225s, episode steps: 14, steps per second: 62, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.098 [-0.941, 1.580], loss: 1.116699, mean_absolute_error: 3.547990, mean_q: 6.512179\n",
      " 1017/5000: episode: 64, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.937, 1.543], loss: 1.011975, mean_absolute_error: 3.565011, mean_q: 6.493340\n",
      " 1031/5000: episode: 65, duration: 0.230s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.074 [-0.994, 1.549], loss: 0.851357, mean_absolute_error: 3.566848, mean_q: 6.644038\n",
      " 1045/5000: episode: 66, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-0.776, 1.468], loss: 1.067124, mean_absolute_error: 3.619937, mean_q: 6.706092\n",
      " 1060/5000: episode: 67, duration: 0.246s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.795, 1.410], loss: 0.900883, mean_absolute_error: 3.635843, mean_q: 6.749562\n",
      " 1071/5000: episode: 68, duration: 0.195s, episode steps: 11, steps per second: 56, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.116 [-0.768, 1.413], loss: 0.812008, mean_absolute_error: 3.633964, mean_q: 6.769662\n",
      " 1083/5000: episode: 69, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-0.966, 1.545], loss: 0.829178, mean_absolute_error: 3.690992, mean_q: 6.974984\n",
      " 1097/5000: episode: 70, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.121 [-0.971, 1.727], loss: 1.067542, mean_absolute_error: 3.740056, mean_q: 6.937821\n",
      " 1107/5000: episode: 71, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.120 [-1.025, 1.601], loss: 1.227652, mean_absolute_error: 3.830023, mean_q: 6.965940\n",
      " 1119/5000: episode: 72, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.125 [-0.569, 1.245], loss: 1.003452, mean_absolute_error: 3.797880, mean_q: 7.009127\n",
      " 1135/5000: episode: 73, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.067 [-0.643, 1.197], loss: 0.983125, mean_absolute_error: 3.850702, mean_q: 7.111703\n",
      " 1151/5000: episode: 74, duration: 0.268s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.119 [-0.554, 1.109], loss: 1.067269, mean_absolute_error: 3.866129, mean_q: 7.105163\n",
      " 1164/5000: episode: 75, duration: 0.212s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.104 [-0.771, 1.437], loss: 0.940309, mean_absolute_error: 3.881678, mean_q: 7.146297\n",
      " 1178/5000: episode: 76, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.121 [-0.555, 1.189], loss: 1.245095, mean_absolute_error: 3.938493, mean_q: 7.170724\n",
      " 1193/5000: episode: 77, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.081 [-0.761, 1.255], loss: 0.862072, mean_absolute_error: 3.944996, mean_q: 7.376000\n",
      " 1205/5000: episode: 78, duration: 0.191s, episode steps: 12, steps per second: 63, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.119 [-0.552, 1.087], loss: 0.947031, mean_absolute_error: 3.992335, mean_q: 7.374369\n",
      " 1227/5000: episode: 79, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.555, 0.995], loss: 0.864293, mean_absolute_error: 4.022713, mean_q: 7.516242\n",
      " 1249/5000: episode: 80, duration: 0.364s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.362, 0.908], loss: 1.089860, mean_absolute_error: 4.100863, mean_q: 7.570709\n",
      " 1269/5000: episode: 81, duration: 0.333s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.096 [-0.400, 1.136], loss: 1.381912, mean_absolute_error: 4.176920, mean_q: 7.609488\n",
      " 1294/5000: episode: 82, duration: 0.414s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.074 [-0.358, 0.875], loss: 1.309143, mean_absolute_error: 4.184259, mean_q: 7.591200\n",
      " 1325/5000: episode: 83, duration: 0.516s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.076 [-0.414, 0.755], loss: 1.159737, mean_absolute_error: 4.233178, mean_q: 7.804292\n",
      " 1357/5000: episode: 84, duration: 0.531s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.346, 0.838], loss: 1.347498, mean_absolute_error: 4.315544, mean_q: 7.893468\n",
      " 1388/5000: episode: 85, duration: 0.514s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.087 [-0.248, 0.750], loss: 1.232734, mean_absolute_error: 4.363671, mean_q: 8.020853\n",
      " 1426/5000: episode: 86, duration: 0.633s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.079 [-0.390, 0.894], loss: 1.159818, mean_absolute_error: 4.443475, mean_q: 8.216218\n",
      " 1520/5000: episode: 87, duration: 1.567s, episode steps: 94, steps per second: 60, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.087 [-0.340, 0.766], loss: 0.963622, mean_absolute_error: 4.596450, mean_q: 8.671091\n",
      " 1573/5000: episode: 88, duration: 0.882s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.132 [-0.689, 0.413], loss: 1.031602, mean_absolute_error: 4.792787, mean_q: 9.033186\n",
      " 1607/5000: episode: 89, duration: 0.565s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.906, 0.369], loss: 1.291643, mean_absolute_error: 4.966793, mean_q: 9.469901\n",
      " 1634/5000: episode: 90, duration: 0.449s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.093 [-1.073, 0.268], loss: 1.218849, mean_absolute_error: 5.099207, mean_q: 9.619633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1654/5000: episode: 91, duration: 0.331s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-0.922, 0.270], loss: 1.017990, mean_absolute_error: 5.092920, mean_q: 9.727821\n",
      " 1678/5000: episode: 92, duration: 0.394s, episode steps: 24, steps per second: 61, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-0.903, 0.199], loss: 1.070331, mean_absolute_error: 5.193445, mean_q: 9.932083\n",
      " 1699/5000: episode: 93, duration: 0.355s, episode steps: 21, steps per second: 59, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.096 [-1.311, 0.601], loss: 1.389282, mean_absolute_error: 5.267002, mean_q: 10.020810\n",
      " 1718/5000: episode: 94, duration: 0.314s, episode steps: 19, steps per second: 61, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.093 [-1.370, 0.585], loss: 1.145872, mean_absolute_error: 5.303315, mean_q: 10.155342\n",
      " 1748/5000: episode: 95, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.014 [-1.133, 0.635], loss: 1.508142, mean_absolute_error: 5.416674, mean_q: 10.296827\n",
      " 1762/5000: episode: 96, duration: 0.237s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.089 [-1.531, 0.841], loss: 1.250100, mean_absolute_error: 5.440662, mean_q: 10.339885\n",
      " 1774/5000: episode: 97, duration: 0.192s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.111 [-1.503, 0.780], loss: 1.624074, mean_absolute_error: 5.504341, mean_q: 10.354030\n",
      " 1796/5000: episode: 98, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.059 [-2.043, 1.212], loss: 1.542334, mean_absolute_error: 5.534882, mean_q: 10.487571\n",
      " 1811/5000: episode: 99, duration: 0.240s, episode steps: 15, steps per second: 62, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.100 [-2.306, 1.366], loss: 1.277743, mean_absolute_error: 5.521597, mean_q: 10.463162\n",
      " 1820/5000: episode: 100, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.297, 1.332], loss: 1.226434, mean_absolute_error: 5.683016, mean_q: 10.935198\n",
      " 1833/5000: episode: 101, duration: 0.215s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.120 [-1.294, 0.609], loss: 1.847191, mean_absolute_error: 5.711287, mean_q: 10.886785\n",
      " 1847/5000: episode: 102, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.108 [-1.270, 0.590], loss: 1.276078, mean_absolute_error: 5.709926, mean_q: 10.933547\n",
      " 1859/5000: episode: 103, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.099 [-1.276, 0.793], loss: 1.630267, mean_absolute_error: 5.793266, mean_q: 11.045219\n",
      " 1880/5000: episode: 104, duration: 0.348s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.073 [-1.267, 0.604], loss: 1.712296, mean_absolute_error: 5.871227, mean_q: 11.139463\n",
      " 1892/5000: episode: 105, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.127 [-2.117, 1.167], loss: 2.415867, mean_absolute_error: 5.960726, mean_q: 11.126068\n",
      " 1904/5000: episode: 106, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.523, 0.787], loss: 1.564988, mean_absolute_error: 5.881764, mean_q: 11.146244\n",
      " 1916/5000: episode: 107, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.098 [-2.037, 1.197], loss: 1.620917, mean_absolute_error: 5.976628, mean_q: 11.372619\n",
      " 1929/5000: episode: 108, duration: 0.207s, episode steps: 13, steps per second: 63, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.086 [-2.319, 1.420], loss: 3.017291, mean_absolute_error: 6.026211, mean_q: 11.204603\n",
      " 1948/5000: episode: 109, duration: 0.323s, episode steps: 19, steps per second: 59, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.061 [-1.392, 0.826], loss: 1.854302, mean_absolute_error: 6.065899, mean_q: 11.405793\n",
      " 1969/5000: episode: 110, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.091 [-0.945, 0.408], loss: 2.721338, mean_absolute_error: 6.214262, mean_q: 11.566511\n",
      " 1991/5000: episode: 111, duration: 0.358s, episode steps: 22, steps per second: 62, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.084 [-1.079, 0.401], loss: 2.118697, mean_absolute_error: 6.234166, mean_q: 11.643902\n",
      " 2016/5000: episode: 112, duration: 0.416s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.082 [-1.181, 0.435], loss: 1.759687, mean_absolute_error: 6.131936, mean_q: 11.635277\n",
      " 2080/5000: episode: 113, duration: 1.074s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.124 [-0.443, 0.877], loss: 2.100050, mean_absolute_error: 6.302294, mean_q: 11.959608\n",
      " 2107/5000: episode: 114, duration: 0.447s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.058, 0.378], loss: 1.801019, mean_absolute_error: 6.376055, mean_q: 12.271682\n",
      " 2133/5000: episode: 115, duration: 0.432s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-0.978, 0.372], loss: 2.981990, mean_absolute_error: 6.624099, mean_q: 12.495794\n",
      " 2194/5000: episode: 116, duration: 1.015s, episode steps: 61, steps per second: 60, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.118 [-0.265, 0.982], loss: 2.014045, mean_absolute_error: 6.661460, mean_q: 12.733116\n",
      " 2214/5000: episode: 117, duration: 0.331s, episode steps: 20, steps per second: 61, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.908, 0.408], loss: 2.190841, mean_absolute_error: 6.716805, mean_q: 12.769404\n",
      " 2238/5000: episode: 118, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-0.971, 0.199], loss: 2.282837, mean_absolute_error: 6.756391, mean_q: 12.866687\n",
      " 2259/5000: episode: 119, duration: 0.353s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.109 [-0.985, 0.234], loss: 2.225225, mean_absolute_error: 6.844155, mean_q: 13.079961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2281/5000: episode: 120, duration: 0.362s, episode steps: 22, steps per second: 61, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.977, 0.268], loss: 3.057518, mean_absolute_error: 7.042519, mean_q: 13.265256\n",
      " 2304/5000: episode: 121, duration: 0.379s, episode steps: 23, steps per second: 61, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.118 [-0.995, 0.357], loss: 3.041288, mean_absolute_error: 6.971515, mean_q: 13.191207\n",
      " 2382/5000: episode: 122, duration: 1.303s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.422, 0.981], loss: 3.087565, mean_absolute_error: 7.055212, mean_q: 13.194709\n",
      " 2408/5000: episode: 123, duration: 0.430s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-0.874, 0.174], loss: 2.822892, mean_absolute_error: 7.142225, mean_q: 13.517715\n",
      " 2460/5000: episode: 124, duration: 0.868s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.112 [-0.506, 0.887], loss: 2.766667, mean_absolute_error: 7.216776, mean_q: 13.658188\n",
      " 2497/5000: episode: 125, duration: 0.603s, episode steps: 37, steps per second: 61, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.076 [-0.791, 0.530], loss: 2.740437, mean_absolute_error: 7.248012, mean_q: 13.813680\n",
      " 2521/5000: episode: 126, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.872, 0.234], loss: 2.956651, mean_absolute_error: 7.344936, mean_q: 13.956731\n",
      " 2549/5000: episode: 127, duration: 0.478s, episode steps: 28, steps per second: 59, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.102 [-0.737, 0.426], loss: 3.029303, mean_absolute_error: 7.406095, mean_q: 14.049810\n",
      " 2604/5000: episode: 128, duration: 0.915s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.026 [-0.927, 0.440], loss: 2.699887, mean_absolute_error: 7.474248, mean_q: 14.256476\n",
      " 2662/5000: episode: 129, duration: 0.965s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.307, 0.984], loss: 2.950741, mean_absolute_error: 7.653649, mean_q: 14.607352\n",
      " 2734/5000: episode: 130, duration: 1.199s, episode steps: 72, steps per second: 60, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.026 [-0.724, 0.376], loss: 2.656023, mean_absolute_error: 7.745623, mean_q: 14.824117\n",
      " 2770/5000: episode: 131, duration: 0.597s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.107 [-0.734, 0.190], loss: 2.948150, mean_absolute_error: 7.832208, mean_q: 14.983911\n",
      " 2826/5000: episode: 132, duration: 0.933s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.043 [-0.709, 0.218], loss: 3.076646, mean_absolute_error: 7.961426, mean_q: 15.204610\n",
      " 2888/5000: episode: 133, duration: 1.031s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.581, 0.985], loss: 2.811921, mean_absolute_error: 8.062207, mean_q: 15.486757\n",
      " 2926/5000: episode: 134, duration: 0.634s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.095 [-0.889, 0.229], loss: 3.170045, mean_absolute_error: 8.094502, mean_q: 15.361606\n",
      " 2954/5000: episode: 135, duration: 0.464s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.109 [-0.803, 0.196], loss: 3.697423, mean_absolute_error: 8.270825, mean_q: 15.749112\n",
      " 3007/5000: episode: 136, duration: 0.885s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.069 [-0.756, 0.357], loss: 2.568318, mean_absolute_error: 8.235100, mean_q: 15.878156\n",
      " 3041/5000: episode: 137, duration: 0.562s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.073, 0.395], loss: 3.162052, mean_absolute_error: 8.398556, mean_q: 16.197487\n",
      " 3083/5000: episode: 138, duration: 0.699s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.095 [-0.771, 0.229], loss: 3.794439, mean_absolute_error: 8.546148, mean_q: 16.293747\n",
      " 3132/5000: episode: 139, duration: 0.815s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.078 [-0.907, 0.352], loss: 3.433787, mean_absolute_error: 8.593360, mean_q: 16.401636\n",
      " 3244/5000: episode: 140, duration: 1.867s, episode steps: 112, steps per second: 60, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.074 [-0.881, 0.596], loss: 2.977891, mean_absolute_error: 8.687769, mean_q: 16.730925\n",
      " 3298/5000: episode: 141, duration: 0.899s, episode steps: 54, steps per second: 60, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.146, 0.208], loss: 4.314891, mean_absolute_error: 8.871270, mean_q: 16.882135\n",
      " 3325/5000: episode: 142, duration: 0.447s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.124 [-0.837, 0.206], loss: 3.233206, mean_absolute_error: 8.897636, mean_q: 17.114811\n",
      " 3373/5000: episode: 143, duration: 0.802s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.152 [-0.339, 0.767], loss: 4.072194, mean_absolute_error: 9.051242, mean_q: 17.346331\n",
      " 3431/5000: episode: 144, duration: 0.964s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.087 [-0.688, 0.492], loss: 3.465422, mean_absolute_error: 9.082202, mean_q: 17.469397\n",
      " 3486/5000: episode: 145, duration: 0.916s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.101 [-0.377, 0.827], loss: 2.965997, mean_absolute_error: 9.199788, mean_q: 17.821100\n",
      " 3522/5000: episode: 146, duration: 0.598s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.147 [-0.642, 0.179], loss: 2.900954, mean_absolute_error: 9.232874, mean_q: 17.835632\n",
      " 3563/5000: episode: 147, duration: 0.682s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.129 [-0.784, 0.165], loss: 3.596960, mean_absolute_error: 9.396180, mean_q: 18.102020\n",
      " 3686/5000: episode: 148, duration: 2.042s, episode steps: 123, steps per second: 60, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.047 [-0.640, 0.432], loss: 3.424914, mean_absolute_error: 9.435436, mean_q: 18.220072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3724/5000: episode: 149, duration: 0.642s, episode steps: 38, steps per second: 59, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.126 [-0.697, 0.418], loss: 3.012249, mean_absolute_error: 9.716666, mean_q: 18.933285\n",
      " 3763/5000: episode: 150, duration: 0.636s, episode steps: 39, steps per second: 61, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.120 [-0.701, 0.195], loss: 3.569971, mean_absolute_error: 9.732076, mean_q: 18.842508\n",
      " 3808/5000: episode: 151, duration: 0.762s, episode steps: 45, steps per second: 59, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.090 [-0.591, 0.207], loss: 3.950249, mean_absolute_error: 9.849675, mean_q: 19.018942\n",
      " 3868/5000: episode: 152, duration: 0.998s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.106 [-0.944, 0.194], loss: 4.899281, mean_absolute_error: 9.866040, mean_q: 18.900051\n",
      " 3922/5000: episode: 153, duration: 0.889s, episode steps: 54, steps per second: 61, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.114 [-0.712, 0.214], loss: 3.789030, mean_absolute_error: 9.914245, mean_q: 19.177071\n",
      " 3972/5000: episode: 154, duration: 0.827s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.115 [-0.745, 0.175], loss: 4.176715, mean_absolute_error: 10.066788, mean_q: 19.480167\n",
      " 4065/5000: episode: 155, duration: 1.549s, episode steps: 93, steps per second: 60, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.085 [-0.524, 0.804], loss: 3.953858, mean_absolute_error: 10.157474, mean_q: 19.662655\n",
      " 4105/5000: episode: 156, duration: 0.671s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.136 [-0.739, 0.372], loss: 3.925231, mean_absolute_error: 10.231134, mean_q: 19.831692\n",
      " 4169/5000: episode: 157, duration: 1.060s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.152 [-0.508, 0.905], loss: 4.764618, mean_absolute_error: 10.333849, mean_q: 19.965765\n",
      " 4203/5000: episode: 158, duration: 0.575s, episode steps: 34, steps per second: 59, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.092 [-0.942, 0.232], loss: 3.748689, mean_absolute_error: 10.298470, mean_q: 20.049025\n",
      " 4268/5000: episode: 159, duration: 1.084s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.032 [-0.793, 0.415], loss: 4.076628, mean_absolute_error: 10.528885, mean_q: 20.449656\n",
      " 4310/5000: episode: 160, duration: 0.698s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.133 [-0.685, 0.252], loss: 4.355402, mean_absolute_error: 10.605042, mean_q: 20.475834\n",
      " 4447/5000: episode: 161, duration: 2.275s, episode steps: 137, steps per second: 60, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.217 [-0.561, 1.087], loss: 3.668778, mean_absolute_error: 10.758582, mean_q: 20.950516\n",
      " 4557/5000: episode: 162, duration: 1.835s, episode steps: 110, steps per second: 60, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.014 [-0.884, 0.593], loss: 4.333531, mean_absolute_error: 10.995713, mean_q: 21.401081\n",
      " 4603/5000: episode: 163, duration: 0.775s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.126 [-0.762, 0.351], loss: 4.950410, mean_absolute_error: 11.168190, mean_q: 21.680458\n",
      " 4672/5000: episode: 164, duration: 1.151s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.176 [-0.453, 0.921], loss: 4.292456, mean_absolute_error: 11.182055, mean_q: 21.737324\n",
      " 4724/5000: episode: 165, duration: 0.865s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.135 [-0.754, 0.190], loss: 4.617002, mean_absolute_error: 11.394091, mean_q: 22.144722\n",
      " 4811/5000: episode: 166, duration: 1.457s, episode steps: 87, steps per second: 60, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.074 [-0.957, 0.347], loss: 3.538648, mean_absolute_error: 11.433983, mean_q: 22.444248\n",
      " 4899/5000: episode: 167, duration: 1.462s, episode steps: 88, steps per second: 60, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.095 [-1.079, 0.327], loss: 4.730057, mean_absolute_error: 11.555086, mean_q: 22.597136\n",
      "done, took 84.037 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef1039def0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 51.000, steps: 51\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
